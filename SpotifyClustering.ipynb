{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Unsupersived Clustering for Spotify\n",
    "The goal of this project is to create a dimensionality reduction technique to do unsupervised clustering of songs in my 'Liked Music' playlist into understanble playlists.\n",
    "\n",
    "Used this guide:\n",
    "* https://medium.com/@dionb/exploring-unsupervised-learning-with-my-spotify-playlist-cdcc3e2ef0b7\n",
    "\n",
    "### Helper Functions\n",
    "We start importing helper functions to be used in the analyze_my_spotify() function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import urllib.parse\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "class Song:\n",
    "    \n",
    "    def __init__(self, name, artists, track_id, features, cluster=\"NONE\"):\n",
    "        \"\"\"name is a string\n",
    "        \n",
    "           artists is a list of artists \n",
    "           \n",
    "           track_id is the unique track_id of a song \n",
    "           \n",
    "           features is a dictionary with attributes:\n",
    "           \n",
    "                 - danceability, energy, loudness, speechiness,\n",
    "                 - acousticness, instrumentalness, liveness,\n",
    "                 - valence, tempo, and time signature\n",
    "                \n",
    "            cluster is a string denoting playlist after clustering (default=\"NONE\")\n",
    "        \"\"\"\n",
    "        self.name=name\n",
    "        self.artists=artists\n",
    "        self.track_id=track_id\n",
    "        self.features=features\n",
    "        self.cluster=cluster\n",
    "        \n",
    "    def __repr__(self):\n",
    "        song_rep = f\"{self.name} by {self.artists}\"\n",
    "        \n",
    "        return song_rep"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class color:\n",
    "   PURPLE = '\\033[95m'\n",
    "   CYAN = '\\033[96m'\n",
    "   DARKCYAN = '\\033[36m'\n",
    "   BLUE = '\\033[94m'\n",
    "   GREEN = '\\033[92m'\n",
    "   YELLOW = '\\033[93m'\n",
    "   RED = '\\033[91m'\n",
    "   BOLD = '\\033[1m'\n",
    "   UNDERLINE = '\\033[4m'\n",
    "   END = '\\033[0m'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Integrating the Spotify Web API."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SpotifyClient(object):\n",
    "    \n",
    "    ### NOTE: integrating Spotify Web API by using a SpotifyClient object\n",
    "    ### and the logic behind get_requests was obtained from the link above :)\n",
    "    ### all else is my code\n",
    "\n",
    "    def __init__(self, api_token):\n",
    "        self.api_token = api_token\n",
    "    \n",
    "    def get_playlist(self, lop):\n",
    "        \n",
    "        def get_requests(url):\n",
    "            request = requests.get(\n",
    "                    url,\n",
    "                    headers={\n",
    "                        \"Content-Type\": \"application/json\",\n",
    "                        \"Authorization\": f\"Bearer {self.api_token}\"\n",
    "                    }\n",
    "                ).json()\n",
    "            \n",
    "            return request\n",
    "        \n",
    "        \n",
    "        \n",
    "        master_lst=[ ] # list of all tracks across lop\n",
    "        \n",
    "        ## Extract the tracks from each playlist\n",
    "        print(\"Extracting Tracks from Playlists\")\n",
    "        print('\\n')\n",
    "        \n",
    "        for playlist in lop:\n",
    "            query = urllib.parse.quote(playlist)\n",
    "            url = f\"https://api.spotify.com/v1/playlists/{query}/tracks\"\n",
    "            playlist_results=get_requests(url)\n",
    "            \n",
    "            # get the unique track ids from the playlist\n",
    "            track_ids=list(set(map(lambda x: x['track']['id'],playlist_results['items'])))\n",
    "            \n",
    "            \n",
    "            # save the track_ids for all songs in lop\n",
    "            master_lst=master_lst+track_ids\n",
    "            \n",
    "        print(f'{len(master_lst)} songs detected. Extracting Track Info')\n",
    "        \n",
    "        ## Now extract features for each track_id in list of track_ids\n",
    "        ## However there is a request limit=100 for song attributes\n",
    "        ## so loop over len(master_lst) times (song and artists are seperate requests)\n",
    "        \n",
    "        feature_set = {\"danceability\", \"energy\", \"loudness\",\"speechiness\",\n",
    "                       \"acousticness\", \"instrumentalness\",\"liveness\", \n",
    "                       \"valence\", \"tempo\"} #\"time_signature\"}\n",
    "        \n",
    "        ## extract the song info\n",
    "        tracks = {}\n",
    "        \n",
    "        print('\\n')\n",
    "        \n",
    "        for bops in master_lst:\n",
    "            url = f\"https://api.spotify.com/v1/audio-features?ids={bops}\"\n",
    "            song_attributes = get_requests(url)['audio_features'][0]\n",
    "            \n",
    "            \n",
    "            url = f\"https://api.spotify.com/v1/tracks?ids={bops}\"\n",
    "            info = get_requests(url)['tracks'][0] #info on song name and artists\n",
    "            artists = list(map(lambda x: x['name'], info['artists']))\n",
    "            \n",
    "            \n",
    "            tracks[bops] = Song(info['name'],artists, bops, \n",
    "                                {feature: song_attributes[feature] for feature in  feature_set})\n",
    "        \n",
    "        #return tracks\n",
    "        #results={'master_lst': master_lst, 'tracks': tracks}\n",
    "        return tracks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**normalize_matrix**\n",
    "\n",
    "Normalizes columns of matrix, D.\n",
    "\n",
    "**dict_to_matrix**\n",
    "\n",
    "Convert dictionary of tracks as a matrix representation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize_matrix(D):\n",
    "    n = D.shape[0]\n",
    "    p = D.shape[1]\n",
    "    \n",
    "    for col in list(range(0,p)):\n",
    "        col_mean = np.mean(D[:,col])\n",
    "        col_var= np.var(D[:,col])\n",
    "        D[:,col] = (D[:,col]-col_mean)/(col_var**0.5)\n",
    "    \n",
    "    return D\n",
    "    \n",
    "\n",
    "def dict_to_matrix(d, normalize=True):\n",
    "    \"\"\"Takes a dictionary d as input and creates an n x p matrix\n",
    "       where n is the sample size and p is the number of covariates\n",
    "       \n",
    "       Takes a boolean value if we wish to normalize the design matrix; default is TRUE\n",
    "       since this is the input to Principal Component Analysis (PCA) and clustering\n",
    "       \n",
    "    \"\"\"\n",
    "    \n",
    "    covariates = [\"danceability\", \"energy\", \"loudness\",\n",
    "                  \"speechiness\",\"acousticness\", \"instrumentalness\",\n",
    "                  \"liveness\", \"valence\", \"tempo\"]#, \"time_signature\"]\n",
    "    matrix_flat = np.array([]) # a flattened matrix (aka array) to which to populate\n",
    "    for bops in d.values():\n",
    "        bop = bops.features\n",
    "        row = np.array([bop[\"danceability\"], bop[\"energy\"], bop[\"loudness\"],\n",
    "                        bop[\"speechiness\"], bop[\"acousticness\"],\n",
    "                        bop[\"instrumentalness\"], bop[\"liveness\"],\n",
    "                        bop[\"valence\"], bop[\"tempo\"]])#, bop[\"time_signature\"]])\n",
    "        \n",
    "        matrix_flat=np.append(matrix_flat, row)\n",
    "    \n",
    "    ## convert flat matrix to matrix (design matrix)\n",
    "    n = len(d)\n",
    "    p = len(covariates)\n",
    "    matrix_dim = matrix_flat.reshape(n,p)\n",
    "    \n",
    "    if normalize:\n",
    "        matrix_dim = normalize_matrix(matrix_dim)\n",
    "        #for col in list(range(0,p)):\n",
    "        #    col_mean = np.mean(matrix_dim[:,col])\n",
    "        #    col_var= np.var(matrix_dim[:,col])\n",
    "        #    matrix_dim[:,col] = (matrix_dim[:,col]-col_mean)/(col_var**0.5)\n",
    "            \n",
    "    return matrix_dim"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PCA \n",
    "Principle component analysis to best distribute our data. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def myPCA(X):\n",
    "    \"\"\"Principal Component Analysis take as input\n",
    "       the centered design matrix, X\n",
    "    \"\"\"\n",
    "    \n",
    "    ### Output the correlation matrix ## \n",
    "    \n",
    "    print(color.BOLD+\"Correlation Matrix\"+color.END)\n",
    "    print('\\n')\n",
    "    \n",
    "    df = pd.DataFrame(X)\n",
    "    df.columns = [\"danceability\", \"energy\", \"loudness\",\n",
    "                  \"speechiness\",\"acousticness\", \"instrumentalness\",\n",
    "                  \"liveness\", \"valence\", \"tempo\"]#, \"time_signature\"]\n",
    "    \n",
    "    plt.figure(figsize=(20,10))\n",
    "    matrix = np.triu(df.corr())\n",
    "    ax=sns.heatmap(df.corr(), annot=True, mask=matrix)\n",
    "    plt.show()\n",
    "    \n",
    "    \n",
    "    \n",
    "    # variance-covariance structure for sample\n",
    "    # X'X where X centered scale by n-1\n",
    "    \n",
    "    \n",
    "    Sigma = np.matmul(np.transpose(X), X)\n",
    "    Sigma_scaled = Sigma/((len(X)-1))\n",
    "    \n",
    "    #########################################\n",
    "    \n",
    "    ####### SVD of X #######    \n",
    "    \n",
    "    #########################################\n",
    "    \n",
    "    ####### diagonalize Sigma = VDV' #######\n",
    "    eigenpairs=np.linalg.eig(Sigma_scaled)\n",
    "    eigenvals = eigenpairs[0]\n",
    "    eigenvecs = eigenpairs[1]\n",
    "    \n",
    "    sorted_eVals = sorted(eigenvals, reverse=True) # lambda_(max),...,lambda_(min)\n",
    "    order = [sorted_eVals.index(i) for i in eigenvals]\n",
    "    diag = np.diag(eigenvals)\n",
    "    eigenvecs = eigenvecs[:, order] # eigen vectors sorted per vals\n",
    "    \n",
    "    ########\n",
    "    \n",
    "    #u = [(1/(x**0.5)) for x in sorted_eVals]\n",
    "    \n",
    "    #U=np.array([])\n",
    "    #for i in list(range(0,len(u))):\n",
    "    #    u_i = u[i]*np.matmul(X,eigenvecs[:,i])\n",
    "    #    U = np.append(U, u_i)\n",
    "    \n",
    "    #U=np.transpose(U.reshape(len(u), len(X))) # no need to extend basis since n<<p\n",
    "    \n",
    "    #########################################\n",
    "    \n",
    "    expl_var = sorted_eVals/sum(sorted_eVals) # explained variance\n",
    "    \n",
    "    print('\\n')\n",
    "    \n",
    "    prop_lst = [ ]\n",
    "    print(color.BOLD+\"Cumulative Explained Variance and Corresponding Eigenvalues:\"+color.END)\n",
    "    for i in list(range(0, len(expl_var))):\n",
    "        prop = round(sum(expl_var[:i+1])/sum(expl_var),3)\n",
    "        print(f\"k={str(i+1)}: [exp_var={str(prop)}, lambda={str(round(sorted_eVals[i],3))}]\")\n",
    "        prop_lst = prop_lst+[prop]\n",
    "    \n",
    "    print('\\n')\n",
    "    plt.figure(figsize=(20,10))\n",
    "    plt.axhline(y=0.9, color='red', linewidth=0.8,linestyle='--')\n",
    "    plt.plot(list(range(1,len(expl_var)+1)),prop_lst, linewidth=2 , marker='8')\n",
    "    plt.title(\"Proportion of Explained Variation (Cumulative)\", fontsize=20)\n",
    "    plt.xlabel('Number of Principal Components Included', fontsize=16)\n",
    "    plt.ylabel('Proportion Explained', fontsize=16)\n",
    "    plt.show()\n",
    "    \n",
    "    print('\\n')\n",
    "    print(\"Based on the diagnostic plots, indicate a new dimension for your data.\")\n",
    "    k = int(input(\"Dimensionality Reduction parameter: \"))\n",
    "    \n",
    "    \n",
    "    ########\n",
    "    \n",
    "    # Reduced data: X = U_k(S_k)V_k' by SVD\n",
    "    \n",
    "    #######\n",
    "    \n",
    "    X_new = np.matmul(X,eigenvecs)\n",
    "    \n",
    "    ## Standardize X_new\n",
    "    \n",
    "    X_new=normalize_matrix(X_new)\n",
    "    \n",
    "    \n",
    "    \n",
    "    # non truncated results\n",
    "    results = {'X_new': X_new,  \n",
    "               'Eigenmat': eigenvecs,\n",
    "               'Eigenvecs':sorted_eVals,\n",
    "               'k':k}\n",
    "    \n",
    "    return results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "get_KNN\n",
    "\n",
    "My implementation of K-Nearest Neighbour"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_KNN(df, k, song_name):\n",
    "    track_lst = dict(zip([x.name.upper() for x in df.index], list(range(0,len(df)))))\n",
    "    df['row']=list(range(0, len(df)))\n",
    "    x = df[df['row']==track_lst[song_name.upper()]]\n",
    "    \n",
    "    n = len(df)\n",
    "    p = len(list(df.columns))-1\n",
    "    \n",
    "    norms_matrix = np.empty((n,p))\n",
    "    for i in list(filter(lambda x: x!='row', list(df.columns))):\n",
    "        norms_matrix[:,i] = (np.array(df[i]) - float(x[i]))**2 # center around song\n",
    "    \n",
    "    \n",
    "    norms_matrix = norms_matrix.reshape(len(df), len(list(df.columns))-1)\n",
    "    norms_vec = norms_matrix.sum(axis=1)**0.5 # get the euclidean dist\n",
    "    track_lst = dict(zip(np.array([x for x in df.index]), norms_vec))\n",
    "    \n",
    "    # sort dictionary per value (i.e, songs per euclidean distance)\n",
    "    track_dict = {key: round(val,2) for key, val in sorted(list(track_lst.items()), key=lambda distance: distance[1])}\n",
    "    return list(track_dict.items())[1:k+1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cluster object and instance methods\n",
    "\n",
    "Binary tree-like representation of a cluster - filled with instance methods to: get euclidean distance, compute centroid, get weights, convert Cluster object into adjacency matrix, and get clusters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Cluster:\n",
    "    \"\"\"a Cluster has the following properties:\n",
    "         - a centroid (a list of length n representing n-dimensional space)\n",
    "         - songs is a set that contains all the track ids of a cluster\n",
    "         - a left child (a Cluster object or False)\n",
    "         - a right child (a Cluster object or False)\n",
    "         - weight (distance between left and right child)\n",
    "         \n",
    "        a cluster of size 1 has left=right=weight=False by default;\n",
    "        \n",
    "        note the left and right child of a cluster is itself a cluster\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, centroid, songs = {},left=False, right=False, weight=False):\n",
    "        self.centroid = centroid\n",
    "        self.songs = songs\n",
    "        self.left = left\n",
    "        self.right = right\n",
    "        self.weight = weight\n",
    "        \n",
    "    \n",
    "        \n",
    "    def __repr__(self):\n",
    "        centr = f\"Centroid at {self.centroid}\"\n",
    "        child = f\"Left Child {self.left}\"\n",
    "        child2 = f\"Right Child {self.right}\"\n",
    "        rep =  centr + '\\n' + child + '\\n' + child2\n",
    "        return rep\n",
    "        \n",
    "    def __eq__(self, other):\n",
    "        return self.left==other.left and self.right==other.right\n",
    "    \n",
    "    def get_euclidean(self, other):\n",
    "        sum_sq = sum(list(map(lambda diff: (diff)**2, (map(lambda x,y: x-y, self.centroid, other.centroid)))))\n",
    "        return round(np.sqrt(sum_sq), 4)\n",
    "    def compute_centroid(self):\n",
    "        \"\"\"compute the cluster's centroid given a tree structure, self\"\"\"\n",
    "        \n",
    "        # get to the root of the tree (aka the individual songs) and get song's centroid\n",
    "        queue = [self]\n",
    "        centroids = []\n",
    "        \n",
    "        while queue != []:\n",
    "            curr = queue.pop(0)\n",
    "            \n",
    "            # base case: if both child are False, then get the centroid\n",
    "            if not (curr.left and curr.right):\n",
    "                centroids.append(curr.centroid)\n",
    "            \n",
    "            else:\n",
    "                queue.append(curr.left)\n",
    "                queue.append(curr.right)\n",
    "            \n",
    "         \n",
    "        # the centroids is a list of lists \n",
    "        # want to sum the i-th element across all n-lists in list\n",
    "        \n",
    "        dim = len(centroids[0]) # determine cluster dimension in R^n\n",
    "        cluster_centroid = list(map(lambda y: np.mean(y), [[x[i] for x in centroids] for i in list(range(0,dim))]))\n",
    "        \n",
    "        results = {'centroid': cluster_centroid, 'children': len(centroids)}\n",
    "        return results\n",
    "    \n",
    "    \n",
    "    \n",
    "    def get_weight(self):\n",
    "        queue = [self]\n",
    "        wt = []\n",
    "        \n",
    "        while queue!=[]:\n",
    "            curr = queue.pop(0)\n",
    "            \n",
    "            if not (curr.left and curr.right):\n",
    "                pass\n",
    "            elif not curr.left:\n",
    "                queue.append(curr.right)\n",
    "                wt.append(curr.weight)\n",
    "                \n",
    "            elif not curr.right:\n",
    "                queue.append(curr.left)\n",
    "                wt.append(curr.weight)\n",
    "                \n",
    "            else:\n",
    "                queue.append(curr.right)\n",
    "                queue.append(curr.left)\n",
    "                wt.append(curr.weight)\n",
    "        \n",
    "        return sorted(wt, reverse=True)\n",
    "    \n",
    "    \n",
    "    def get_sse(self):\n",
    "        \"\"\"takes a cluster as input and computes the \n",
    "           sum squared variation between the cluster\n",
    "           centroid and all the members\n",
    "        \"\"\"\n",
    "        \n",
    "        # get to the root of the tree (aka the individual songs) and get song's centroid\n",
    "        queue = [self]\n",
    "        centroids = [] # of members\n",
    "        \n",
    "        while queue != []:\n",
    "            curr = queue.pop(0)\n",
    "            \n",
    "            # base case: if both child are False, then get the centroid\n",
    "            if not (curr.left and curr.right):\n",
    "                #centroids.append(curr.centroid)\n",
    "                dist = curr.get_euclidean(self)**2\n",
    "                centroids.append(dist)\n",
    "            else:\n",
    "                queue.append(curr.left)\n",
    "                queue.append(curr.right)\n",
    "        #dim = len(centroids[0]) # determine cluster dimension in R^n\n",
    "        master_centroid = self.centroid\n",
    "        #sse = #sum(list(map(lambda x: x**2,centroids)))\n",
    "        \n",
    "        #sum(list(map(lambda x: sum((x.get_euclidean(self))**2), centroids)))\n",
    "        \n",
    "        return np.sqrt(sum(centroids))\n",
    "    \n",
    "    \n",
    "    def count_children(self):\n",
    "        queue = [self]\n",
    "        centroids = [] # of members\n",
    "        \n",
    "        while queue != []:\n",
    "            curr = queue.pop(0)\n",
    "            \n",
    "            # base case: if both child are False, then get the centroid\n",
    "            if not (curr.left and curr.right):\n",
    "                #centroids.append(curr.centroid)\n",
    "                dist = curr.get_euclidean(self)**2\n",
    "                centroids.append(dist)\n",
    "            else:\n",
    "                queue.append(curr.left)\n",
    "                queue.append(curr.right)\n",
    "                \n",
    "    def cluster_to_adjacency(self):\n",
    "        edges ={}\n",
    "        queue = [self]\n",
    "        \n",
    "        while queue!=[]:\n",
    "            curr = queue.pop(0)\n",
    "            key = \"-\".join(sorted([x for x in curr.songs]))\n",
    "            edges[key]={}\n",
    "            \n",
    "            if type(curr.left)!=type(True): #self has a left child\n",
    "                child = curr.left\n",
    "                child_name = \"-\".join(sorted([x for x in child.songs]))\n",
    "                edges[key][child_name]=curr.weight\n",
    "                queue.append(child)\n",
    "            if type(curr.left)!=type(True): #self has a right child\n",
    "                child = curr.right\n",
    "                child_name = \"-\".join(sorted([x for x in child.songs]))\n",
    "                edges[key][child_name]=curr.weight\n",
    "                queue.append(child)\n",
    "            \n",
    "        return edges\n",
    "    \n",
    "    def get_clusters(self, wt, k):\n",
    "        queue = [self]\n",
    "        clusters = []\n",
    "        \n",
    "        while len(clusters)<k:\n",
    "            curr=queue.pop(0)\n",
    "            #print(queue, len(clusters))\n",
    "            \n",
    "            if curr.weight == wt:\n",
    "                clusters.append(curr)\n",
    "            \n",
    "            elif curr.weight>wt:\n",
    "                if curr.left.weight < wt:\n",
    "                    clusters.append(curr.left)\n",
    "                else:\n",
    "                    queue.append(curr.left)\n",
    "                \n",
    "                if curr.right.weight < wt:\n",
    "                    clusters.append(curr.right)\n",
    "                \n",
    "                else:\n",
    "                    queue.append(curr.right)\n",
    "                \n",
    "                #:\n",
    "                #    queue.append(curr.left)\n",
    "                #    queue.append(curr.right)\n",
    "                    \n",
    "            else:\n",
    "                queue.append(curr.left)\n",
    "                queue.append(curr.right)\n",
    "        \n",
    "        return clusters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cluster by \n",
    "## A CENTROID IMPLEMENTATION ##\n",
    "\n",
    "def cluster_by_centroids(tracks_mat):\n",
    "    \n",
    "    # define a local function that gets the nearest\n",
    "    # cluster (neighbour) between self and the cluster_queue\n",
    "    #def get_NN(x, cluster_queue):\n",
    "    #    cluster_dist = [np.sqrt(sum((y-x)**2)) for y in cluster_queue]\n",
    "    #    closest_nbr = sorted(cluster_dist)\n",
    "    #    return cluster_dist.index(closest_nbr[0])\n",
    "    \n",
    "    # define the roots of the tree:\n",
    "    cluster_queue = [Cluster(x, False, False, False) for x in tracks_mat]\n",
    "    \n",
    "    \n",
    "    # nested clustering loop\n",
    "    while len(cluster_queue)>1:\n",
    "        \n",
    "        #curr = cluster_queue.pop(0)\n",
    "        current_min = 100**100 # an arbitrarily large number\n",
    "        #current_max = -current_min\n",
    "        cluster_1 = None \n",
    "        cluster_2 = None\n",
    "        n = len(cluster_queue)\n",
    "        \n",
    "        # find the closest pairwise cluster\n",
    "        for i in list(range(0, n)):\n",
    "            curr = cluster_queue[i]\n",
    "            for j in list(range(i+1,n)):\n",
    "                other = cluster_queue[j]\n",
    "                euclidean = curr.get_euclidean(other)\n",
    "                \n",
    "                if euclidean < current_min:\n",
    "                    current_min = euclidean\n",
    "                    cluster_1 = curr\n",
    "                    cluster_2 = other\n",
    "                    i_star = i\n",
    "                    j_star = j-1\n",
    "                \n",
    "                #if euclidean > current_max:\n",
    "                #    current_max = euclidean\n",
    "        \n",
    "        # remove the clusters to be merged from queue\n",
    "        cluster_queue.pop(i_star)\n",
    "        cluster_queue.pop(j_star)\n",
    "        \n",
    "        # merge two closest cluster\n",
    "        merged = Cluster(0, # centroid = 0 is a placeholder to be updated\n",
    "                        cluster_1, \n",
    "                        cluster_2,\n",
    "                        current_min, current_max)\n",
    "        \n",
    "        merged.centroid = merged.compute_centroid()\n",
    "        \n",
    "        # add the merged cluster into the queue\n",
    "        cluster_queue.append(merged)\n",
    "    \n",
    "    # return the parent node (from which the entire tree can be deduced)\n",
    "    return merged"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "cluster_by_variability\n",
    "\n",
    "Hierarchical clustering using ward's linkage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "## MINIMIZE PAIRWISE CLUSTER VARIANCE ##\n",
    "\n",
    "def cluster_by_variability(tracks_mat, song_labels):\n",
    "    \n",
    "    # define a local function that gets the nearest\n",
    "    # cluster (neighbour) between self and the cluster_queue\n",
    "    #def get_NN(x, cluster_queue):\n",
    "    #    cluster_dist = [np.sqrt(sum((y-x)**2)) for y in cluster_queue]\n",
    "    #    closest_nbr = sorted(cluster_dist)\n",
    "    #    return cluster_dist.index(closest_nbr[0])\n",
    "    \n",
    "    # define the roots of the tree:\n",
    "    cluster_queue = [Cluster(tracks_mat[i], {song_labels[i].track_id}, False, False, False)\n",
    "                     for i in list(range(len(song_labels)))]\n",
    "    \n",
    "    \n",
    "    #return cluster_queue\n",
    "    # nested clustering loop\n",
    "    while len(cluster_queue)>1:\n",
    "        \n",
    "        #curr = cluster_queue.pop(0)\n",
    "        current_min = 100**100 # an arbitrarily large number\n",
    "        cluster_1 = None \n",
    "        cluster_2 = None\n",
    "        n = len(cluster_queue)\n",
    "        \n",
    "        # find the closest pairwise cluster\n",
    "        for i in list(range(0, n)):\n",
    "            curr = cluster_queue[i]\n",
    "            n_i = curr.compute_centroid()['children']\n",
    "            \n",
    "            for j in list(range(i+1,n)):\n",
    "                other = cluster_queue[j]\n",
    "                \n",
    "                \n",
    "                n_j = other.compute_centroid()['children']\n",
    "                delta_ij = round(((n_i*n_j)/(n_i+n_j))*sum(((np.array(curr.centroid)-np.array(other.centroid))**2)),4)\n",
    "                \n",
    "                # save the info about the two clusters to be merged\n",
    "                if delta_ij < current_min:\n",
    "                    current_min = delta_ij\n",
    "                    cluster_1 = curr\n",
    "                    cluster_2 = other\n",
    "                    i_star = i\n",
    "                    j_star = j-1\n",
    "        \n",
    "        # remove the clusters to be merged from queue\n",
    "        \n",
    "        #print(current_min, cluster_1.songs, cluster_2.songs)\n",
    "        cluster_queue.pop(i_star)\n",
    "        cluster_queue.pop(j_star)\n",
    "        \n",
    "        \n",
    "        # merge two closest clusters\n",
    "        merged = Cluster(0, # centroid = 0 is a placeholder to be updated\n",
    "                        cluster_1.songs.union(cluster_2.songs),\n",
    "                        cluster_1, \n",
    "                        cluster_2,\n",
    "                        current_min)\n",
    "        \n",
    "        merged.centroid = merged.compute_centroid()['centroid']\n",
    "        \n",
    "        # add the merged cluster into the queue\n",
    "        cluster_queue.append(merged)\n",
    "    \n",
    "    # return the parent node (from which the entire tree can be deduced)\n",
    "    return merged"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Running Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyze_my_spotify():\n",
    "    # user credential input\n",
    "    print(color.BOLD+'Hello.'+color.END)\n",
    "    \n",
    "    print('\\n')\n",
    "    print('Please obtain a request token from https://spoti.fi/31fCKVz using your Spotify credentials. ')\n",
    "    \n",
    "    token = input(\"Enter your token: \")\n",
    "    \n",
    "    \n",
    "    s=SpotifyClient(token)\n",
    "    \n",
    "    print('\\n')\n",
    "    \n",
    "    print('Please enter the spotify playlist ID seperating by commas: ')\n",
    "    playlist_ids = input(\"Playlist URIs: \")\n",
    "    playlist_ids = list(map(lambda x: x.strip(),playlist_ids.split(\",\")))\n",
    "    tracks = []\n",
    "    for i in range(14):\n",
    "        tracks += s.get_playlist(playlist_ids, offset = i*100)\n",
    "    \n",
    "    #playlist_ids=[\"2WuJPdcCpLONytcEggSkSC\"]#'3L2HToam3LDyxwd3vkWID0',\"6GDCfyYIfhTdc6QUwOlZdE\"]\n",
    "    \n",
    "    print('\\n')\n",
    "    print(\"Finished Extracting Track Information.\")\n",
    "    \n",
    "    \n",
    "    X=dict_to_matrix(tracks)\n",
    "    \n",
    "    feature_lst = [\"danceability\", \"energy\", \"loudness\",\n",
    "                  \"speechiness\",\"acousticness\", \"instrumentalness\",\n",
    "                  \"liveness\", \"valence\", \"tempo\", \"time_signature\"]\n",
    "    \n",
    "    # plot the marginal distribution of variables\n",
    "    #print('Exploratory Plots: ')\n",
    "    \n",
    "    #fig, axes = plt.subplots(nrows=1, ncols=2, figsize=(10, 10))\n",
    "    #axes[0].distplot(X[:,0],hist=False, rug=True, color=\"firebrick\")\n",
    "    #axes[1].hist(X[:,1])\n",
    "    #fig.tight_layout()\n",
    "    \n",
    "    \n",
    "    print('\\n')\n",
    "    # start PCA\n",
    "    print(color.BOLD+color.BLUE+\"Principal Component Analysis\"+color.END)\n",
    "    print('\\n')\n",
    "    pca_results = myPCA(X)\n",
    "    # the transformed design matrix, X under a different basis \n",
    "    df=pd.DataFrame(pca_results['X_new'], index=tracks.values())\n",
    "    \n",
    "    print('\\n')\n",
    "    # Plot the data in 2D and 1D\n",
    "    \n",
    "    print('Would you like to plot the data in 2-dimensions (Using the 2 Principal Components)?')\n",
    "    plot_in2d = input('Y/N: ')\n",
    "    \n",
    "    if plot_in2d.upper().strip()=='Y':\n",
    "        norms=[x for x in np.sqrt((df[0]**2)+(df[1]**2))]\n",
    "        cols = [norms.index(i) for i in norms if round(i,3)<=1.96] #inside the neigborhood\n",
    "        cols2 = list(filter(lambda x: x not in cols, list(range(0,len(df)))))\n",
    "        \n",
    "        plt.figure(figsize=(20,10))\n",
    "        plt.scatter(df[0][cols], df[1][cols], marker='o', linewidth=5, alpha=0.8, label=\"delta=1.96 nbrhood of (0,0)\")\n",
    "        plt.scatter(df[0][cols2],df[1][cols2],marker='*', linewidth=5,alpha=0.8, color=\"firebrick\")\n",
    "        \n",
    "        for x in cols2: \n",
    "            plt.annotate(x, (df[0][x], df[1][x]), fontsize=17)\n",
    "        \n",
    "        plt.xlabel(\"First Principal Component\", fontsize=15)\n",
    "        plt.ylabel(\"Second Principal Component\", fontsize=15)\n",
    "        plt.legend(loc=2, fontsize=14)\n",
    "        plt.title(f\"Variability Explained = {round(sum(pca_results['Eigenvecs'][0:2])/sum(pca_results['Eigenvecs']),3)}\", fontsize=18)\n",
    "        print('\\n')\n",
    "        print(color.UNDERLINE+'2-Dimensional Representation of the Playlist:' + color.END)\n",
    "        plt.show()\n",
    "        \n",
    "        not_nhbr=pd.DataFrame([], index=cols2,columns = [\"Song\", \"PC1\",\"PC2\"])\n",
    "\n",
    "        track_names = list(tracks.values())\n",
    "        for x in cols2:\n",
    "            not_nhbr.at[x,'Song']=track_names[x]\n",
    "            not_nhbr.at[x,'PC1']=df[0][x]\n",
    "            not_nhbr.at[x,'PC2']=df[1][x]\n",
    "\n",
    "        print(not_nhbr)\n",
    "        print('\\n')\n",
    "                  \n",
    "    print('\\n')\n",
    "    # Find the nearest neighbors of the average\n",
    "    print(color.BOLD+ color.BLUE +'K-Nearest Neighbours' +color.END)\n",
    "\n",
    "    print('Given a song name in the playlist, K-Nearest Neighbours gives K songs that are most similar to the inputted song in the playlist.')\n",
    "    print('\\n')\n",
    "    print('Would you like to proceed with looking at the KNN of songs in your playlist?')\n",
    "    print('Y - proceed with KNN; N - proceed directly to clustering.')\n",
    "    print('\\n')\n",
    "    Knn_bool = input('Y/N: ').upper().strip()\n",
    "    print('\\n')\n",
    "\n",
    "    if Knn_bool=='Y':\n",
    "        boole = True\n",
    "        track_lst = {x.name.upper() for x in df.index}\n",
    "    else:\n",
    "        boole = False\n",
    "                  \n",
    "    def prompt_knn():\n",
    "        print('\\n')\n",
    "        print(\"Please enter the song name as it appears on the spotify playlist.\")\n",
    "        song_name = input(\"Song name: \").strip()\n",
    "        if song_name.upper() not in track_lst:\n",
    "            print(f\"{song_name} was not found in the inputted playlist. Please enter a valid song.\")\n",
    "            prompt_knn()\n",
    "        else:\n",
    "            print(f\"How many neighbours around {song_name}?\")\n",
    "            k = int(input(f\"k nearest neighbours around {song_name} with k = \").strip())\n",
    "            print(pd.DataFrame(get_KNN(df, k, song_name), columns = ['Song', 'Euclidean Distance']).to_string())\n",
    "                  \n",
    "                  \n",
    "    while boole:\n",
    "        prompt_knn()\n",
    "        #if s!=None:\n",
    "        #    print(pd.DataFrame(get_KNN(df, s[1], s[0]), columns = ['Song', 'Euclidean Distance']).to_string())\n",
    "        print('\\n')\n",
    "        print('Would you like to look at the Nearest Neighbours of more songs?')\n",
    "        b=input('Y/N: ').upper().strip()\n",
    "        if b != 'Y':\n",
    "            boole = False\n",
    "    \n",
    "    print('\\n')\n",
    "    #print('Terminating KNN.')\n",
    "    print('\\n')\n",
    "    # Cluster\n",
    "                  \n",
    "    print(color.BLUE+color.BOLD+'Agglomerative Hierarchical Clustering' + color.END)\n",
    "    print('\\n')\n",
    "    print('Clustering Playlist Using Within Cluster Variability as a Dissimilarity Measure')\n",
    "    clustering=cluster_by_variability(pca_results['X_new'][:,0:pca_results['k']], df.index)\n",
    "    print('\\n')\n",
    "    print(color.UNDERLINE+'Clustering Results'+color.END)\n",
    "    wts = clustering.get_weight()\n",
    "    wts.append(0)\n",
    "    \n",
    "    print('\\n')\n",
    "    \n",
    "    plt.figure(figsize=(20,10))\n",
    "    plt.plot(list(range(1, len(wts)+1)),wts, marker='o')\n",
    "    plt.xlabel('Number of Clusters',fontsize=18)\n",
    "    plt.ylabel('Merging Cost', fontsize =18)\n",
    "    plt.title('Number of Clusters vs Dissimilarity', fontsize=20)\n",
    "    plt.show()\n",
    "                  \n",
    "    print('\\n')\n",
    "    print(f'Select a number of clusters, k, where k is an integer from 1 to {len(df)}')\n",
    "    cluster_no = int(input(\"Number of Clusters: \").strip())\n",
    "    \n",
    "    # Extracting playlists\n",
    "    print('\\n')\n",
    "    print(\"Extracting Playlists\")\n",
    "    playlist_clusters = clustering.get_clusters(wts[cluster_no-1], cluster_no)\n",
    "    \n",
    "                  \n",
    "    track_df=pd.DataFrame(tracks.items(),columns=['track_id', 'name'])\n",
    "    #track_df['track_id']=track_df['track_id'].astype('str')\n",
    "    track_df['artists']=track_df['name'].apply(lambda x: '-'.join(x.artists))\n",
    "    track_df['name']=track_df['name'].apply(lambda x: x.name)\n",
    "\n",
    "    print('\\n')\n",
    "            \n",
    "    i=1\n",
    "    for playlists in playlist_clusters:\n",
    "        print(color.BOLD+ f'Playlist {i}' +color.END)\n",
    "        playlst = pd.DataFrame(playlists.songs, columns=['track_id'])\n",
    "        playlst[f'Cluster_{i}'] = int(i)\n",
    "        songs = [tracks[x] for x in playlists.songs]\n",
    "                  \n",
    "        for x in songs:\n",
    "            print(x)\n",
    "                  \n",
    "        print('\\n')\n",
    "        track_df = pd.merge(track_df,playlst, on='track_id',how='left')\n",
    "        print('\\n')\n",
    "        print('\\n')\n",
    "        i=i+1\n",
    "                  \n",
    "    track_df.fillna(0, inplace=True)\n",
    "    track_df['Cluster']=track_df.sum(axis=1)\n",
    "    track_df = track_df[['track_id', 'name','artists','Cluster']]\n",
    "    \n",
    "    #return track_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 2.1: Analyze your own spotify.\n",
    "Re-run the cell below and follow the prompts. I left a sample output:\n",
    "\n",
    "Notes:\n",
    "\n",
    "To get your spotify credentials go to: https://spoti.fi/31fCKVz, click get token, check playlist-read-private, and click request token (Note that credentials expire every 1 hour)\n",
    "\n",
    "Each playlist should only have 100 songs since that is the limit (so if you have a playlist of over 100 + songs, need to seperate them manually into 100-song playlists).\n",
    "\n",
    "The playlist ID is the last 22 strings of the spotify url. https://open.spotify.com/playlist/3L2HToam3LDyxwd3vkWID0 => 3L2HToam3LDyxwd3vkWID0\n",
    "\n",
    "Any further questions or comments? Reach out to my LinkedIn @ https://www.linkedin.com/in/dion-banno-78b955192/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1mHello.\u001b[0m\n",
      "\n",
      "\n",
      "Please obtain a request token from https://spoti.fi/31fCKVz using your Spotify credentials. \n",
      "Enter your token: BQBKSkET9N8P3ZOj_Guvg5dcRt0wwoJCQqSa7SkYKUSkCW0m-rAE9CGjRrLUr-PsQClm3j-1SoTSuy0ULjnT7Z0Q7G6PNguecr_64_Go9VkaRrVz_kPTX2Io8-ehWST5ujugzUT-KdS1P03g_lE21a22-fkKUx12rbKji9VlZTI\n",
      "\n",
      "\n",
      "Please enter the spotify playlist ID seperating by commas: \n",
      "Playlist URIs: 3y8Bvywc6mJ7Oc06xak8We\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "get_playlist() got an unexpected keyword argument 'offset'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-19-7acbf97be9b9>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0manalyze_my_spotify\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-18-7285fa00002c>\u001b[0m in \u001b[0;36manalyze_my_spotify\u001b[1;34m()\u001b[0m\n\u001b[0;32m     18\u001b[0m     \u001b[0mtracks\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     19\u001b[0m     \u001b[1;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m14\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 20\u001b[1;33m         \u001b[0mtracks\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[0ms\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_playlist\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mplaylist_ids\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0moffset\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mi\u001b[0m\u001b[1;33m*\u001b[0m\u001b[1;36m100\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     21\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     22\u001b[0m     \u001b[1;31m#playlist_ids=[\"2WuJPdcCpLONytcEggSkSC\"]#'3L2HToam3LDyxwd3vkWID0',\"6GDCfyYIfhTdc6QUwOlZdE\"]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mTypeError\u001b[0m: get_playlist() got an unexpected keyword argument 'offset'"
     ]
    }
   ],
   "source": [
    "analyze_my_spotify()\n",
    "# BQBKSkET9N8P3ZOj_Guvg5dcRt0wwoJCQqSa7SkYKUSkCW0m-rAE9CGjRrLUr-PsQClm3j-1SoTSuy0ULjnT7Z0Q7G6PNguecr_64_Go9VkaRrVz_kPTX2Io8-ehWST5ujugzUT-KdS1P03g_lE21a22-fkKUx12rbKji9VlZTI\n",
    "# 3y8Bvywc6mJ7Oc06xak8We"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
